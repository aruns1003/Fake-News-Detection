{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb82274-d1e1-4e54-9eec-831ba5b0ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      4733\n",
      "           1       0.98      0.99      0.98      4247\n",
      "\n",
      "    accuracy                           0.98      8980\n",
      "   macro avg       0.98      0.98      0.98      8980\n",
      "weighted avg       0.98      0.98      0.98      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"C:/Users/aruns/Downloads/archive/Fake.csv\")\n",
    "df['label'] = 0\n",
    "true_df = pd.read_csv(\"C:/Users/aruns/Downloads/archive/True.csv\")\n",
    "true_df['label'] = 1\n",
    "data = pd.concat([df, true_df])\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'], data['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Logistic Regression baseline\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d8df255-dca7-4b1d-93eb-d6171e4da6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     -------------------------------------- 42.0/42.0 kB 675.1 kB/s eta 0:00:00\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.34-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.9.5)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.3 MB 7.5 MB/s eta 0:00:02\n",
      "    --------------------------------------- 0.2/11.3 MB 2.5 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.4/11.3 MB 2.7 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.6/11.3 MB 3.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/11.3 MB 3.9 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.2/11.3 MB 4.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/11.3 MB 4.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/11.3 MB 4.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/11.3 MB 4.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/11.3 MB 3.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.4/11.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.4/11.3 MB 2.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.4/11.3 MB 2.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.4/11.3 MB 2.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.4/11.3 MB 2.2 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 2.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 2.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.5/11.3 MB 2.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.7/11.3 MB 2.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.8/11.3 MB 2.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.9/11.3 MB 2.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.9/11.3 MB 2.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.9/11.3 MB 2.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.9/11.3 MB 2.0 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.9/11.3 MB 2.0 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 2.5 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 2.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.9/11.3 MB 2.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 3.0/11.3 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.2/11.3 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.5/11.3 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.5/11.3 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.5/11.3 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 3.9/11.3 MB 2.3 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.3/11.3 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.8/11.3 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.5/11.3 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.7/11.3 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.2/11.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.6/11.3 MB 3.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.9/11.3 MB 3.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.2/11.3 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.2/11.3 MB 3.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.6/11.3 MB 3.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.8/11.3 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.3/11.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.3/11.3 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.8/11.3 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.4/11.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.8/11.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.1/11.3 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.4/11.3 MB 4.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.9/11.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.3 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.3 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "   ---------------------------------------- 0.0/494.8 kB ? eta -:--:--\n",
      "   ---------------------------- ---------- 358.4/494.8 kB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 494.8/494.8 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "   ---------------------------------------- 0.0/561.5 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 419.8/561.5 kB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 561.5/561.5 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading pyarrow-21.0.0-cp39-cp39-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/26.2 MB 4.3 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.3/26.2 MB 3.5 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.4/26.2 MB 4.5 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.7/26.2 MB 3.8 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.7/26.2 MB 2.9 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 1.1/26.2 MB 4.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 1.4/26.2 MB 4.5 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.8/26.2 MB 4.9 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.2/26.2 MB 5.2 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.4/26.2 MB 5.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.4/26.2 MB 5.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.4/26.2 MB 5.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.4/26.2 MB 4.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.5/26.2 MB 3.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.5/26.2 MB 3.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 2.6/26.2 MB 3.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.7/26.2 MB 3.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.7/26.2 MB 3.5 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.9/26.2 MB 3.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.3/26.2 MB 3.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.4/26.2 MB 3.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.6/26.2 MB 3.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.6/26.2 MB 3.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.7/26.2 MB 3.3 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.8/26.2 MB 3.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 3.9/26.2 MB 3.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.0/26.2 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.0/26.2 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.0/26.2 MB 3.0 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.1/26.2 MB 3.0 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.1/26.2 MB 2.9 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.2/26.2 MB 2.8 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.3/26.2 MB 2.8 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.4/26.2 MB 2.8 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.5/26.2 MB 2.8 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.5/26.2 MB 2.7 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 4.6/26.2 MB 2.7 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 4.6/26.2 MB 2.6 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 4.9/26.2 MB 2.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 5.4/26.2 MB 2.9 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 5.6/26.2 MB 3.0 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 6.2/26.2 MB 3.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 6.5/26.2 MB 3.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 6.5/26.2 MB 3.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.7/26.2 MB 3.2 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.9/26.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 7.1/26.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 7.2/26.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 7.5/26.2 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 7.9/26.2 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 8.4/26.2 MB 3.6 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 9.0/26.2 MB 3.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.4/26.2 MB 3.8 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.7/26.2 MB 3.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.7/26.2 MB 3.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.7/26.2 MB 3.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.7/26.2 MB 3.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.7/26.2 MB 3.7 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 9.7/26.2 MB 3.7 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 9.9/26.2 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.0/26.2 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.0/26.2 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.2/26.2 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.4/26.2 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.4/26.2 MB 3.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.4/26.2 MB 3.4 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.4/26.2 MB 3.4 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 10.5/26.2 MB 3.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.5/26.2 MB 3.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.6/26.2 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.7/26.2 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.7/26.2 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 10.9/26.2 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 11.1/26.2 MB 3.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 11.2/26.2 MB 3.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 11.4/26.2 MB 3.1 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 11.4/26.2 MB 3.1 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 11.7/26.2 MB 3.1 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.9/26.2 MB 3.1 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 12.0/26.2 MB 3.1 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 12.0/26.2 MB 3.0 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 12.0/26.2 MB 3.0 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 12.1/26.2 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 12.1/26.2 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 12.1/26.2 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.6/26.2 MB 2.9 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.8/26.2 MB 3.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 13.0/26.2 MB 3.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 13.0/26.2 MB 3.1 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 13.0/26.2 MB 3.1 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.2/26.2 MB 3.0 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.2/26.2 MB 3.0 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.5/26.2 MB 3.0 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.5/26.2 MB 3.0 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.5/26.2 MB 3.0 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.5/26.2 MB 2.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.5/26.2 MB 2.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.5/26.2 MB 2.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.6/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.6/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.8/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.9/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 14.0/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 14.0/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 14.1/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 14.2/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 14.2/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 14.3/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 14.4/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.5/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.5/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.5/26.2 MB 2.8 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.8/26.2 MB 2.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.1/26.2 MB 2.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.2/26.2 MB 2.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.4/26.2 MB 2.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.5/26.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.7/26.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.7/26.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.8/26.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 16.0/26.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 16.1/26.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 16.1/26.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 16.1/26.2 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 16.1/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 16.1/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 16.4/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.6/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 17.1/26.2 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 17.4/26.2 MB 2.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 17.7/26.2 MB 2.7 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 18.2/26.2 MB 2.7 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 18.6/26.2 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.6/26.2 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.8/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 19.2/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 19.3/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.7/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 19.9/26.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 20.0/26.2 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 20.0/26.2 MB 2.8 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 20.2/26.2 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 20.7/26.2 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.8/26.2 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.2/26.2 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.4/26.2 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 21.5/26.2 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.7/26.2 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.0/26.2 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.0/26.2 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.0/26.2 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.0/26.2 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.0/26.2 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.1/26.2 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.1/26.2 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.1/26.2 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.1/26.2 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.1/26.2 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.1/26.2 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.1/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.1/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.2/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.2/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.2/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.2/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 22.3/26.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.3/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.5/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.7/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.9/26.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.1/26.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.2/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.4/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.4/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.4/26.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 23.4/26.2 MB 2.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.6/26.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.7/26.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.7/26.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.7/26.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.7/26.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.8/26.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.8/26.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.0/26.2 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.4/26.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.6/26.2 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.8/26.2 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.0/26.2 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.6/26.2 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/26.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.1/26.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading regex-2025.7.34-cp39-cp39-win_amd64.whl (276 kB)\n",
      "   ---------------------------------------- 0.0/276.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 276.1/276.1 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "   ---------------------------------------- 0.0/320.2 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 174.1/320.2 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 320.2/320.2 kB 4.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.2/2.5 MB 10.2 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 0.2/2.5 MB 10.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.5 MB 2.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.5/2.5 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.7/2.5 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.0/2.5 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.4/2.5 MB 4.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.7/2.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.9/2.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.2/2.5 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.3/2.5 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 4.4 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, safetensors, regex, pyarrow, huggingface-hub, tokenizers, transformers, datasets\n",
      "Successfully installed datasets-4.0.0 huggingface-hub-0.34.4 pyarrow-21.0.0 regex-2025.7.34 safetensors-0.6.2 tokenizers-0.21.4 transformers-4.55.4 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e8d3675-f9ca-47c7-9b9f-16f36a49b5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipywidgets) (8.15.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: backcall in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\aruns\\anaconda3\\envs\\gpu\\lib\\site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.8 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 41.0/139.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 122.9/139.8 kB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ - 133.1/139.8 kB 877.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- 139.8/139.8 kB 925.5 kB/s eta 0:00:00\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "   ---------------------------------------- 0.0/216.6 kB ? eta -:--:--\n",
      "   ---------------------------------------  215.0/216.6 kB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 216.6/216.6 kB 6.7 MB/s eta 0:00:00\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 8.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 5.1 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 0.5/2.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.2 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 4.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 6.1 MB/s eta 0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0e9731-2e22-4b63-b560-db2613525a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: unknown command \"install--upgrade\" - maybe you meant \"install\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install--upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc763ea-181e-4c7b-b468-0f855033a74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'evaluate_during_training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m DistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# 4. Training Arguments\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluate_during_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# 5. Trainer\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_recall_fscore_support\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'evaluate_during_training'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 1. Load Dataset\n",
    "fake = pd.read_csv(\"C:/Users/aruns/Downloads/archive/Fake.csv\")\n",
    "true = pd.read_csv(\"C:/Users/aruns/Downloads/archive/True.csv\")\n",
    "\n",
    "fake['label'] = 0\n",
    "true['label'] = 1\n",
    "\n",
    "df = pd.concat([fake, true]).sample(frac=1).reset_index(drop=True)  # shuffle\n",
    "df = df[['text', 'label']]   # keep only relevant columns\n",
    "\n",
    "# Train/test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Tokenization\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': train_labels\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'labels': test_labels\n",
    "})\n",
    "\n",
    "# 3. Model Setup\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 4. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluate_during_training=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# 5. Trainer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 6. Train Model\n",
    "trainer.train()\n",
    "\n",
    "# 7. Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "# Save Model\n",
    "trainer.save_model(\"./distilbert-fake-news-model\")\n",
    "tokenizer.save_pretrained(\"./distilbert-fake-news-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee0b279a-c71b-46f3-bd78-dc9cc7fcbaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 4490/4490 [4:40:09<00:00,  3.74s/it, loss=0.000243]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4490/4490 [3:39:38<00:00,  2.94s/it, loss=5.49e-5]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.0044\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4754\n",
      "           1       1.00      1.00      1.00      4226\n",
      "\n",
      "    accuracy                           1.00      8980\n",
      "   macro avg       1.00      1.00      1.00      8980\n",
      "weighted avg       1.00      1.00      1.00      8980\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./distilbert-fake-news-model\\\\tokenizer_config.json',\n",
       " './distilbert-fake-news-model\\\\special_tokens_map.json',\n",
       " './distilbert-fake-news-model\\\\vocab.txt',\n",
       " './distilbert-fake-news-model\\\\added_tokens.json',\n",
       " './distilbert-fake-news-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load Dataset\n",
    "# -----------------------------\n",
    "fake = pd.read_csv(\"C:/Users/aruns/Downloads/archive/Fake.csv\")\n",
    "true = pd.read_csv(\"C:/Users/aruns/Downloads/archive/True.csv\")\n",
    "\n",
    "fake['label'] = 0\n",
    "true['label'] = 1\n",
    "df = pd.concat([fake, true]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Tokenizer\n",
    "# -----------------------------\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=256)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "dataset = NewsDataset(encodings, labels)\n",
    "\n",
    "# Train/test split\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Model\n",
    "# -----------------------------\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Training Loop\n",
    "# -----------------------------\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Evaluation\n",
    "# -----------------------------\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "preds, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        preds.extend(predictions.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(classification_report(true_labels, preds))\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Save Model\n",
    "# -----------------------------\n",
    "model.save_pretrained(\"./distilbert-fake-news-model\")\n",
    "tokenizer.save_pretrained(\"./distilbert-fake-news-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8b81b-dce6-48ef-b52f-8076178e1c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
